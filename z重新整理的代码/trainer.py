"""
è®­ç»ƒæ¨¡å—ï¼šåˆ†ç¦»å¼è®­ç»ƒç­–ç•¥
ä¸‰ä¸ªç‹¬ç«‹ä¼˜åŒ–å™¨åˆ†åˆ«ä¼˜åŒ–ä¸‰ä¸ªåˆ†æ”¯
"""

import time
import copy
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from tqdm import tqdm
import swanlab

from sklearn.metrics import (
    precision_recall_fscore_support,
    cohen_kappa_score,
    confusion_matrix,
    classification_report,
    roc_auc_score
)


class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""
    def __init__(self, classes=50, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.classes = classes
        self.confidence = 1.0 - smoothing
        
    def forward(self, pred, target):
        """
        pred: [N, C] é¢„æµ‹logits
        target: [N] æ ‡ç­¾
        """
        pred = pred.log_softmax(dim=-1)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.classes - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=-1))


def train_model(model, dataloaders, num_epochs=500, precision=1e-8, device='cuda', 
                use_swanlab=True, swanlab_project='Gesture-Recognition', subject=10, 
                dataset='DB2', experiment_id='M0_base',
                add_test_ratio=0.25, use_branch_loss=True,
                use_disentangle=False, disentangle_config=None,
                use_adaptive_fusion=False, adaptive_fusion_config=None,
                save_predictions=False):
    """
    ç«¯åˆ°ç«¯è®­ç»ƒç­–ç•¥
    
    è®­ç»ƒç‰¹ç‚¹:
        1. ç»Ÿä¸€çš„AdamWä¼˜åŒ–å™¨
        2. å¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨å•æ¨¡æ€åˆ†æ”¯æŸå¤±
        3. æœ€ç»ˆé¢„æµ‹ä½¿ç”¨ä¸‰è·¯åˆ†æ•°åŠ æƒæ±‚å’Œ
        4. å¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨è§£çº ç¼ æŸå¤±
        5. å¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”èåˆæŸå¤±ï¼ˆåˆ›æ–°ç‚¹2ï¼‰
        
    Args:
        model: å¤šæ¨¡æ€æ¨¡å‹
        dataloaders: {'train': train_loader, 'val': val_loader}
        num_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
        precision: æ—©åœç²¾åº¦
        device: è®­ç»ƒè®¾å¤‡
        use_swanlab: æ˜¯å¦ä½¿ç”¨SwanLabç›‘æ§
        swanlab_project: SwanLabé¡¹ç›®åç§°
        subject: å—è¯•è€…ç¼–å·
        dataset: æ•°æ®é›†åç§° ('DB2', 'DB3', 'DB5', 'DB7')
        experiment_id: å®éªŒIDï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼Œå¦‚'M0_base', 'HP_a0.3_b0.5'ï¼‰
        add_test_ratio: ä»æµ‹è¯•é›†æ·»åŠ åˆ°è®­ç»ƒé›†çš„æ¯”ä¾‹
        use_branch_loss: æ˜¯å¦ä½¿ç”¨å•æ¨¡æ€åˆ†æ”¯æŸå¤±ï¼ˆé»˜è®¤Trueï¼‰
        use_disentangle: æ˜¯å¦ä½¿ç”¨è§£çº ç¼ æŸå¤±ï¼ˆé»˜è®¤Falseï¼‰
        disentangle_config: è§£çº ç¼ é…ç½®å¯¹è±¡ï¼ˆå¦‚æœuse_disentangle=Trueåˆ™å¿…é¡»æä¾›ï¼‰
        use_adaptive_fusion: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”èåˆæŸå¤±ï¼ˆé»˜è®¤Falseï¼‰
        adaptive_fusion_config: è‡ªé€‚åº”èåˆé…ç½®å¯¹è±¡ï¼ˆå¦‚æœuse_adaptive_fusion=Trueåˆ™å¿…é¡»æä¾›ï¼‰
        save_predictions: æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœï¼ˆç”¨äºåç»­èšåˆåˆ†æï¼Œé»˜è®¤Falseï¼‰
        
    Returns:
        best_weights: æœ€ä½³æ¨¡å‹æƒé‡
    """
    since = time.time()
    best_acc = float('-inf')
    patience = 20  # åˆå§‹patience
    patience_increase = 15  # æ‰¾åˆ°æ›´å¥½æ¨¡å‹æ—¶å¢åŠ çš„patience
    no_improve_count = 0  # æœªæ”¹å–„è®¡æ•°å™¨
    best_weights = copy.deepcopy(model.state_dict())
    
    # åˆå§‹åŒ–SwanLab
    if use_swanlab:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        loss_strategy = "fusion_only" if not use_branch_loss else "all_branches"
        if use_adaptive_fusion:
            model_type = "WithAdaptiveFusion"
        elif use_disentangle:
            model_type = "WithDisentangle"
        else:
            model_type = "Base"
        
        swanlab_config = {
            "model": f"MultimodalGestureNet{model_type}",
            "dataset": dataset,
            "subject": subject,
            "experiment_id": experiment_id,
            "num_epochs": num_epochs,
            "optimizer": "AdamW",
            "learning_rate": 0.0005,
            "weight_decay": 0.05,
            "betas": "(0.9, 0.999)",
            "dropout": 0.5,
            "label_smoothing": 0.1,
            "device": str(device),
            "training_strategy": "unified_end_to_end",
            "use_branch_loss": use_branch_loss,
            "loss_strategy": loss_strategy,
            "regularization": "L2 + Dropout + LabelSmoothing",
            # æ•°æ®é…ç½®
            "add_test_to_train_ratio": add_test_ratio,
            "data_augmentation": f"{add_test_ratio*100:.0f}% test data added to train",
            # æ¨¡å‹æ¶æ„
            "encoder_channels": "[400, 128, 64, 32]",
            "mlp_layers": "2 (input â†’ 128 â†’ 50)",
            "estimated_params": "~6M"
        }
        
        # æ·»åŠ è§£çº ç¼ é…ç½®
        if use_disentangle and disentangle_config is not None:
            swanlab_config.update({
                "use_disentangle": True,
                "d_shared": disentangle_config.d_shared,
                "d_private": disentangle_config.d_private,
                "lambda1": disentangle_config.lambda1,
                "lambda2": disentangle_config.lambda2,
                "lambda3": disentangle_config.lambda3,
                "lambda4": disentangle_config.lambda4,
                "lambda5": disentangle_config.lambda5,
                "alpha": disentangle_config.alpha,
                "beta": disentangle_config.beta,
                "temperature": disentangle_config.temperature,
                "warmup_epochs": disentangle_config.warmup_epochs
            })
        
        # æ·»åŠ è‡ªé€‚åº”èåˆé…ç½®
        if use_adaptive_fusion and adaptive_fusion_config is not None:
            swanlab_config.update({
                "use_adaptive_fusion": True,
                "unified_dim": adaptive_fusion_config.unified_dim,
                "router_hidden_dim": adaptive_fusion_config.router_hidden_dim,
                "router_dropout": adaptive_fusion_config.router_dropout,
                "router_temperature": adaptive_fusion_config.temperature,
                "lambda_align": adaptive_fusion_config.lambda_align,
                "lambda_balance": adaptive_fusion_config.lambda_balance,
                "balance_type": adaptive_fusion_config.balance_type,
                "fusion_warmup_epochs": adaptive_fusion_config.warmup_epochs
            })
        
        swanlab.init(
            project=swanlab_project,
            experiment_name=f"{dataset}_S{subject}_{experiment_id}_{timestamp}",
            description=f"{dataset} - å—è¯•è€…{subject} - {experiment_id} - æŸå¤±: {loss_strategy}",
            config=swanlab_config
        )
    
    # æŸå¤±å‡½æ•°ï¼šä½¿ç”¨æ ‡ç­¾å¹³æ»‘é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆæ ¹æ®æ•°æ®é›†è‡ªåŠ¨ç¡®å®šç±»åˆ«æ•°ï¼‰
    from config import get_dataset_config
    num_classes = get_dataset_config(dataset)['num_class']
    criterion = LabelSmoothingLoss(classes=num_classes, smoothing=0.1).to(device)
    
    # è§£çº ç¼ æŸå¤±å‡½æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    disentangle_loss_fn = None
    if use_disentangle:
        if disentangle_config is None:
            raise ValueError("use_disentangle=True ä½†æœªæä¾› disentangle_config")
        from models.disentangle_loss import DisentangleLoss
        from config import get_dataset_config
        
        # è®¡ç®—ç‰¹å¾ç»´åº¦ï¼ˆç”¨äºConditionalCLUBï¼‰
        dataset_cfg = get_dataset_config(dataset)
        emg_feature_dim = 32 * dataset_cfg['emg_channels']  # åŠ¨æ€
        imu_feature_dim = 32 * dataset_cfg['imu_channels']  # åŠ¨æ€
        
        disentangle_loss_fn = DisentangleLoss(
            disentangle_config, 
            emg_feature_dim=emg_feature_dim,
            imu_feature_dim=imu_feature_dim
        ).to(device)
        print(f"âœ“ è§£çº ç¼ æŸå¤±å·²å¯ç”¨")
        print(f"  - ç‰¹å¾ç»´åº¦: EMG={emg_feature_dim}, IMU={imu_feature_dim}")
        print(f"  - å…±äº«ç»´åº¦: {disentangle_config.d_shared}, ç‹¬ç‰¹ç»´åº¦: {disentangle_config.d_private}")
        print(f"  - æƒé‡: Î±={disentangle_config.alpha}, Î²={disentangle_config.beta}")
    
    # è‡ªé€‚åº”èåˆæŸå¤±å‡½æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    adaptive_fusion_loss_fn = None
    if use_adaptive_fusion:
        if adaptive_fusion_config is None:
            raise ValueError("use_adaptive_fusion=True ä½†æœªæä¾› adaptive_fusion_config")
        from models.adaptive_fusion import AdaptiveFusionLoss
        adaptive_fusion_loss_fn = AdaptiveFusionLoss(adaptive_fusion_config).to(device)
        print(f"âœ“ è‡ªé€‚åº”èåˆæŸå¤±å·²å¯ç”¨")
        print(f"  - ç»Ÿä¸€ç»´åº¦: {adaptive_fusion_config.unified_dim}")
        print(f"  - æƒé‡: Î»_align={adaptive_fusion_config.lambda_align}, Î»_balance={adaptive_fusion_config.lambda_balance}")
    
    # ç»Ÿä¸€ä¼˜åŒ–å™¨ï¼šAdamWï¼ˆè”åˆä¼˜åŒ–æ‰€æœ‰å‚æ•°ï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=0.0005,             # é™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        betas=(0.9, 0.999),
        eps=1e-8,
        weight_decay=0.05      # å¢å¤§L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šReduceLROnPlateau
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer=optimizer, 
        mode='max', 
        factor=0.5,            # å­¦ä¹ ç‡è¡°å‡å› å­
        patience=10,           # å®¹å¿10ä¸ªepochä¸æå‡
        eps=precision
    )
    
    # å¼€å§‹è®­ç»ƒ
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        print(f'\nEpoch {epoch}/{num_epochs - 1}')
        print('-' * 50)
        
        # æ¯ä¸ªepochéƒ½æœ‰è®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()
            
            running_corrects = 0
            running_corrects_emg = 0
            running_corrects_imu = 0
            running_loss = 0.0
            running_loss_private = 0.0  # è§£çº ç¼ ï¼šç‹¬ç‰¹æŸå¤±
            running_loss_shared = 0.0   # è§£çº ç¼ ï¼šå…±äº«æŸå¤±
            running_loss_adaptive = 0.0  # è‡ªé€‚åº”èåˆï¼šæ€»æŸå¤±
            total = 0
            
            # æ·»åŠ è¿›åº¦æ¡
            pbar = tqdm(dataloaders[phase], 
                       desc=f'{phase.capitalize()}',
                       ncols=80,
                       leave=False,
                       dynamic_ncols=False,
                       position=0)
            
            # éå†æ•°æ®
            for batch_data in pbar:
                # å…¼å®¹æ–°æ—§æ•°æ®æ ¼å¼ï¼ˆæœ‰/æ— exerciseå­—æ®µï¼‰
                if len(batch_data) == 4:
                    emg_inputs, imu_inputs, labels, exercises = batch_data
                else:
                    emg_inputs, imu_inputs, labels = batch_data
                
                emg_inputs = Variable(emg_inputs.to(device))
                imu_inputs = Variable(imu_inputs.to(device))
                labels = Variable(labels.to(device))
                
                # æ¸…é›¶æ¢¯åº¦
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                with torch.set_grad_enabled(phase == 'train'):
                    # ========== æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„å‰å‘ä¼ æ’­è·¯å¾„ ==========
                    if use_adaptive_fusion:
                        # ===== è‡ªé€‚åº”èåˆæ¨¡å‹ï¼ˆå®Œæ•´ç‰ˆï¼šè§£çº ç¼  + è‡ªé€‚åº”èåˆï¼‰=====
                        # å‰å‘ä¼ æ’­ï¼ˆè¿”å›æ‰€æœ‰ä¸­é—´è¡¨å¾ï¼‰
                        total_score, outputs = model(
                            emg_inputs, imu_inputs, 
                            return_all=True
                        )
                        
                        # æå–å„åˆ†æ”¯åˆ†æ•°
                        emg_score = outputs['emg_score']
                        imu_score = outputs['imu_score']
                        fusion_score = outputs['fusion_score']
                        
                        # è®¡ç®—åˆ†ç±»æŸå¤±
                        if use_branch_loss:
                            L_cls = (criterion(emg_score, labels) + 
                                    criterion(imu_score, labels) + 
                                    criterion(fusion_score, labels))
                        else:
                            L_cls = criterion(fusion_score, labels)
                        
                        # åˆå§‹åŒ–å„é¡¹æŸå¤±
                        L_private = torch.tensor(0.0, device=device)
                        L_shared = torch.tensor(0.0, device=device)
                        L_adaptive = torch.tensor(0.0, device=device)
                        
                        # è®¡ç®—è§£çº ç¼ æŸå¤±ï¼ˆwarmupåæ‰å¯ç”¨ï¼‰
                        if epoch >= disentangle_config.warmup_epochs:
                            L_private, L_shared, disentangle_loss_dict = disentangle_loss_fn(
                                outputs, labels
                            )
                        
                        # è®¡ç®—è‡ªé€‚åº”èåˆæŸå¤±ï¼ˆè‡ªé€‚åº”èåˆwarmupåæ‰å¯ç”¨ï¼‰
                        if epoch >= adaptive_fusion_config.warmup_epochs:
                            L_adaptive, adaptive_loss_dict = adaptive_fusion_loss_fn(
                                model.adaptive_fusion
                            )
                        
                        # æ€»æŸå¤±
                        if epoch >= disentangle_config.warmup_epochs and epoch >= adaptive_fusion_config.warmup_epochs:
                            # å®Œæ•´æŸå¤±ï¼šåˆ†ç±» + è§£çº ç¼  + è‡ªé€‚åº”èåˆ
                            total_loss = (L_cls + 
                                        disentangle_config.alpha * L_private + 
                                        disentangle_config.beta * L_shared +
                                        L_adaptive)
                        elif epoch >= disentangle_config.warmup_epochs:
                            # åªæœ‰åˆ†ç±» + è§£çº ç¼ 
                            total_loss = (L_cls + 
                                        disentangle_config.alpha * L_private + 
                                        disentangle_config.beta * L_shared)
                        else:
                            # Warmupé˜¶æ®µï¼šåªä½¿ç”¨åˆ†ç±»æŸå¤±
                            total_loss = L_cls
                    
                    elif use_disentangle:
                        # ===== è§£çº ç¼ æ¨¡å‹ï¼ˆä¸å«è‡ªé€‚åº”èåˆï¼‰=====
                        # å‰å‘ä¼ æ’­ï¼ˆè¿”å›è§£çº ç¼ è¡¨å¾ï¼‰
                        total_score, disentangle_outputs = model(
                            emg_inputs, imu_inputs, 
                            return_disentangle=True
                        )
                        
                        # æå–å„åˆ†æ”¯åˆ†æ•°
                        emg_score = disentangle_outputs['emg_score']
                        imu_score = disentangle_outputs['imu_score']
                        fusion_score = disentangle_outputs['fusion_score']
                        
                        # è®¡ç®—åˆ†ç±»æŸå¤±
                        if use_branch_loss:
                            L_cls = (criterion(emg_score, labels) + 
                                    criterion(imu_score, labels) + 
                                    criterion(fusion_score, labels))
                        else:
                            L_cls = criterion(fusion_score, labels)
                        
                        # è®¡ç®—è§£çº ç¼ æŸå¤±ï¼ˆwarmupåæ‰å¯ç”¨ï¼‰
                        if epoch >= disentangle_config.warmup_epochs:
                            L_private, L_shared, loss_dict = disentangle_loss_fn(
                                disentangle_outputs, labels
                            )
                            
                            # æ€»æŸå¤±
                            total_loss = (L_cls + 
                                        disentangle_config.alpha * L_private + 
                                        disentangle_config.beta * L_shared)
                        else:
                            # Warmupé˜¶æ®µï¼šåªä½¿ç”¨åˆ†ç±»æŸå¤±
                            total_loss = L_cls
                            L_private = torch.tensor(0.0)
                            L_shared = torch.tensor(0.0)
                            loss_dict = {}
                    
                    else:
                        # ===== åŸæœ‰æ¨¡å‹ï¼ˆä¸ä½¿ç”¨è§£çº ç¼ ï¼‰=====
                        # ç‰¹å¾æå–
                        imu_feature, _, _ = model.imu_encoder(imu_inputs)
                        emg_feature, _, _ = model.emg_encoder(emg_inputs)
                        
                        # åˆ†ç±»
                        imu_score = model.imu_classifier(imu_feature)
                        emg_score = model.emg_classifier(emg_feature)
                        
                        # èåˆï¼ˆç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¸ä½¿ç”¨detachï¼‰
                        fusion_feature = model.fusion_encoder(imu_feature, emg_feature)
                        fusion_score = model.fusion_classifier(fusion_feature)
                        
                        # è®¡ç®—æŸå¤±
                        if use_branch_loss:
                            # ä½¿ç”¨ä¸‰ä¸ªåˆ†æ”¯çš„æŸå¤±ï¼ˆé»˜è®¤ï¼‰
                            imu_loss = criterion(imu_score, labels)
                            emg_loss = criterion(emg_score, labels)
                            fusion_loss = criterion(fusion_score, labels)
                            total_loss = imu_loss + emg_loss + fusion_loss
                        else:
                            # åªä½¿ç”¨èåˆåˆ†æ”¯çš„æŸå¤±ï¼ˆæ¶ˆèå®éªŒï¼‰
                            fusion_loss = criterion(fusion_score, labels)
                            total_loss = fusion_loss
                    
                    # åå‘ä¼ æ’­ï¼ˆç»Ÿä¸€ä¼˜åŒ–ï¼‰
                    if phase == 'train':
                        total_loss.backward()
                        optimizer.step()
                    
                    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸‰è·¯åŠ æƒæ±‚å’Œï¼‰
                    if not use_disentangle and not use_adaptive_fusion:
                        total_score = imu_score + emg_score + fusion_score
                    # å¦‚æœä½¿ç”¨è§£çº ç¼ æˆ–è‡ªé€‚åº”èåˆï¼Œtotal_scoreå·²ç»åœ¨forwardä¸­è®¡ç®—
                    _, predictions = torch.max(total_score.data, 1)
                    running_corrects += torch.sum(predictions == labels.data)
                    
                    # è®¡ç®—EMGå’ŒIMUåˆ†æ”¯çš„å•ç‹¬å‡†ç¡®ç‡
                    _, emg_predictions = torch.max(emg_score.data, 1)
                    _, imu_predictions = torch.max(imu_score.data, 1)
                    running_corrects_emg += torch.sum(emg_predictions == labels.data)
                    running_corrects_imu += torch.sum(imu_predictions == labels.data)
                    
                    running_loss += total_loss.item() * labels.size(0)
                    total += labels.size(0)
                    
                    # ç´¯ç§¯è§£çº ç¼ æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if use_disentangle and epoch >= disentangle_config.warmup_epochs:
                        running_loss_private += L_private.item() * labels.size(0)
                        running_loss_shared += L_shared.item() * labels.size(0)
                    
                    # ç´¯ç§¯è‡ªé€‚åº”èåˆæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if use_adaptive_fusion and epoch >= adaptive_fusion_config.warmup_epochs:
                        running_loss_adaptive += L_adaptive.item() * labels.size(0)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    current_acc = (running_corrects.item() / total) * 100.0
                    current_loss = running_loss / total
                    pbar.set_postfix({'Loss': f'{current_loss:.4f}', 'Acc': f'{current_acc:.2f}%'})
            
            # å…³é—­è¿›åº¦æ¡
            pbar.close()
            
            # è®¡ç®—epochç»Ÿè®¡
            epoch_acc = (running_corrects.item() / total) * 100.0
            epoch_acc_emg = (running_corrects_emg.item() / total) * 100.0
            epoch_acc_imu = (running_corrects_imu.item() / total) * 100.0
            epoch_loss = running_loss / total
            
            # è®°å½•åˆ°SwanLab
            if use_swanlab:
                if phase == 'train':
                    log_dict = {
                        'train_loss': epoch_loss,
                        'train_acc': epoch_acc,
                        'train_acc_emg': epoch_acc_emg,
                        'train_acc_imu': epoch_acc_imu,
                        'learning_rate': optimizer.param_groups[0]['lr']
                    }
                    # æ·»åŠ è§£çº ç¼ æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if use_disentangle and epoch >= disentangle_config.warmup_epochs:
                        log_dict.update({
                            'train_loss_private': running_loss_private / total,
                            'train_loss_shared': running_loss_shared / total
                        })
                    # æ·»åŠ è‡ªé€‚åº”èåˆæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if use_adaptive_fusion and epoch >= adaptive_fusion_config.warmup_epochs:
                        log_dict.update({
                            'train_loss_adaptive': running_loss_adaptive / total
                        })
                    swanlab.log(log_dict, step=epoch)
                    train_acc = epoch_acc
                    train_loss = epoch_loss
                else:
                    log_dict = {
                        'val_loss': epoch_loss,
                        'val_acc': epoch_acc,
                        'val_acc_emg': epoch_acc_emg,
                        'val_acc_imu': epoch_acc_imu
                    }
                    # æ·»åŠ è§£çº ç¼ æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if use_disentangle and epoch >= disentangle_config.warmup_epochs:
                        log_dict.update({
                            'val_loss_private': running_loss_private / total,
                            'val_loss_shared': running_loss_shared / total
                        })
                    # æ·»åŠ è‡ªé€‚åº”èåˆæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if use_adaptive_fusion and epoch >= adaptive_fusion_config.warmup_epochs:
                        log_dict.update({
                            'val_loss_adaptive': running_loss_adaptive / total
                        })
                    swanlab.log(log_dict, step=epoch)
            
            # æ¸…æ™°åœ°æ‰“å°ç»“æœï¼ˆç¡®ä¿ä¸ä¼šè¢«è¦†ç›–ï¼‰
            print(f'{phase} Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')
            print(f'  EMGåˆ†æ”¯: {epoch_acc_emg:.2f}%, IMUåˆ†æ”¯: {epoch_acc_imu:.2f}%')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if phase == 'val':
                # è·å–è°ƒåº¦å‰çš„å­¦ä¹ ç‡
                old_lr = optimizer.param_groups[0]['lr']
                
                scheduler.step(epoch_acc)
                
                # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦å˜åŒ–
                new_lr = optimizer.param_groups[0]['lr']
                if new_lr != old_lr:
                    print(f"ğŸ“‰ å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.6f} -> {new_lr:.6f}")
                
                if epoch_acc > best_acc:
                    print(f"âœ“ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {epoch_acc:.2f}% (æå‡: +{epoch_acc - best_acc:.2f}%)")
                    best_acc = epoch_acc
                    best_weights = copy.deepcopy(model.state_dict())
                    patience = patience_increase + epoch
                    no_improve_count = 0  # é‡ç½®æœªæ”¹å–„è®¡æ•°
                    
                    # è®°å½•æœ€ä½³å‡†ç¡®ç‡
                    if use_swanlab:
                        swanlab.log({
                            'best_accuracy': best_acc,
                            'best_epoch': epoch
                        }, step=epoch)
                else:
                    no_improve_count += 1
                    if no_improve_count > 0:
                        print(f"âš ï¸  éªŒè¯å‡†ç¡®ç‡æœªæå‡ (è¿ç»­{no_improve_count}æ¬¡)")
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºepochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        print(f'â±ï¸  Epochæ—¶é—´: {epoch_time:.2f}ç§’')
        
        # è®°å½•epochæ—¶é—´
        if use_swanlab:
            swanlab.log({'epoch_time': epoch_time}, step=epoch)
        
        # æ—©åœ
        if epoch > patience:
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch}åœæ­¢è®­ç»ƒ")
            break
    
    # è®­ç»ƒå®Œæˆ
    time_elapsed = time.time() - since
    print('=' * 50)
    print(f'è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ {time_elapsed // 60:.0f}åˆ† {time_elapsed % 60:.0f}ç§’')
    print(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%')
    
    # è®°å½•è®­ç»ƒæ€»ç»“
    if use_swanlab:
        swanlab.log({
            'final_best_accuracy': best_acc,
            'total_training_time_minutes': time_elapsed / 60
        })
        swanlab.finish()
    
    # ==================== è¯„ä¼°æœ€ä½³æ¨¡å‹ ====================
    print('=' * 50)
    print('æ­£åœ¨è¯„ä¼°æœ€ä½³æ¨¡å‹...')
    model.load_state_dict(best_weights)
    
    metrics = evaluate_best_model(model, dataloaders['val'], device, subject, dataset, experiment_id,
                                  save_predictions=save_predictions)
    print(f'è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ ./results/{dataset}/subject{subject}/{experiment_id}/')
    
    return best_weights


def evaluate_best_model(model, dataloader, device, subject, dataset='DB2', experiment_id='M0_base', save_predictions=False):
    """
    è¯„ä¼°æœ€ä½³æ¨¡å‹çš„å®Œæ•´æŒ‡æ ‡
    
    åŒ…æ‹¬ï¼š
    - æ€§èƒ½æŒ‡æ ‡ï¼šAccuracy, Precision, Recall, F1, Cohen's Kappa, AUROC, Top-3/5 Accuracy
    - æ•ˆç‡æŒ‡æ ‡ï¼šParams, FLOPs, Inference Time
    - å¯è§†åŒ–ï¼šæ··æ·†çŸ©é˜µ
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        dataloader: éªŒè¯é›†æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        subject: å—è¯•è€…ç¼–å·
        dataset: æ•°æ®é›†åç§°
        experiment_id: å®éªŒID
        save_predictions: æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœï¼ˆç”¨äºåç»­èšåˆï¼‰
    
    Returns:
        metrics: å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
    """
    model.eval()
    
    # ç”¨äºæ”¶é›†é¢„æµ‹ç»“æœ
    all_labels = []
    all_preds_fusion = []
    all_preds_emg = []
    all_preds_imu = []
    all_probs_fusion = []  # ç”¨äºTop-kè®¡ç®—å’ŒAUROC
    all_probs_emg = []     # ç”¨äºEMGåˆ†æ”¯AUROC
    all_probs_imu = []     # ç”¨äºIMUåˆ†æ”¯AUROC
    all_exercises = []  # ç”¨äºåˆ†exerciseç»Ÿè®¡
    
    print("æ”¶é›†é¢„æµ‹ç»“æœ...")
    with torch.no_grad():
        for batch_data in tqdm(dataloader, desc="è¯„ä¼°ä¸­"):
            # å…¼å®¹æ–°æ—§æ•°æ®æ ¼å¼ï¼ˆæœ‰/æ— exerciseå­—æ®µï¼‰
            if len(batch_data) == 4:
                emg_inputs, imu_inputs, labels, exercises = batch_data
                all_exercises.extend(exercises.cpu().numpy())
            else:
                emg_inputs, imu_inputs, labels = batch_data
                all_exercises.extend(np.zeros(len(labels), dtype=np.int32))  # é»˜è®¤å€¼0
            
            emg_inputs = emg_inputs.to(device)
            imu_inputs = imu_inputs.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­ï¼ˆå…¼å®¹åŸºç¡€æ¨¡å‹ã€è§£çº ç¼ æ¨¡å‹å’Œè‡ªé€‚åº”èåˆæ¨¡å‹ï¼‰
            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            is_adaptive_fusion_model = hasattr(model, 'adaptive_fusion')
            is_disentangle_model = hasattr(model, 'emg_shared_encoder') and not is_adaptive_fusion_model
            
            if is_adaptive_fusion_model:
                # è‡ªé€‚åº”èåˆæ¨¡å‹ï¼šè°ƒç”¨forward with return_all=True
                total_score, outputs = model(emg_inputs, imu_inputs, return_all=True)
                emg_score = outputs['emg_score']
                imu_score = outputs['imu_score']
                fusion_score = outputs['fusion_score']
            elif is_disentangle_model:
                # è§£çº ç¼ æ¨¡å‹ï¼šè°ƒç”¨forward with return_disentangle=True
                total_score, disentangle_outputs = model(emg_inputs, imu_inputs, return_disentangle=True)
                emg_score = disentangle_outputs['emg_score']
                imu_score = disentangle_outputs['imu_score']
                fusion_score = disentangle_outputs['fusion_score']
            else:
                # åŸºç¡€æ¨¡å‹ï¼šåŸæœ‰é€»è¾‘
                imu_feature, _, _ = model.imu_encoder(imu_inputs)
                emg_feature, _, _ = model.emg_encoder(emg_inputs)
                fusion_feature = model.fusion_encoder(imu_feature, emg_feature)
                
                imu_score = model.imu_classifier(imu_feature)
                emg_score = model.emg_classifier(emg_feature)
                fusion_score = model.fusion_classifier(fusion_feature)
                
                # ä¸‰è·¯èåˆ
                total_score = imu_score + emg_score + fusion_score
            
            # æ”¶é›†ç»“æœ
            all_labels.extend(labels.cpu().numpy())
            all_preds_fusion.extend(torch.argmax(total_score, dim=1).cpu().numpy())
            all_preds_emg.extend(torch.argmax(emg_score, dim=1).cpu().numpy())
            all_preds_imu.extend(torch.argmax(imu_score, dim=1).cpu().numpy())
            all_probs_fusion.append(torch.softmax(total_score, dim=1).cpu().numpy())
            all_probs_emg.append(torch.softmax(emg_score, dim=1).cpu().numpy())
            all_probs_imu.append(torch.softmax(imu_score, dim=1).cpu().numpy())
    
    all_labels = np.array(all_labels)
    all_preds_fusion = np.array(all_preds_fusion)
    all_preds_emg = np.array(all_preds_emg)
    all_preds_imu = np.array(all_preds_imu)
    all_probs_fusion = np.concatenate(all_probs_fusion, axis=0)
    all_probs_emg = np.concatenate(all_probs_emg, axis=0)
    all_probs_imu = np.concatenate(all_probs_imu, axis=0)
    all_exercises = np.array(all_exercises)
    
    # æ¸…ç†GPUç¼“å­˜ï¼Œä¸ºæ¨ç†æ—¶é—´æµ‹é‡è…¾å‡ºç©ºé—´
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ==================== è®¡ç®—æ€§èƒ½æŒ‡æ ‡ ====================
    print("\nè®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    
    def compute_metrics(labels, preds, probs=None):
        """è®¡ç®—å•ä¸ªåˆ†æ”¯çš„æŒ‡æ ‡"""
        # åŸºæœ¬æŒ‡æ ‡
        acc = (preds == labels).mean() * 100
        
        # Precision, Recall, F1
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            labels, preds, average='macro', zero_division=0
        )
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            labels, preds, average='weighted', zero_division=0
        )
        
        # Cohen's Kappa
        kappa = cohen_kappa_score(labels, preds)
        
        # Top-kå‡†ç¡®ç‡å’ŒAUROC
        if probs is not None:
            top3_correct = sum([1 if label in np.argsort(prob)[-3:] else 0 
                               for label, prob in zip(labels, probs)])
            top5_correct = sum([1 if label in np.argsort(prob)[-5:] else 0 
                               for label, prob in zip(labels, probs)])
            top3_acc = (top3_correct / len(labels)) * 100
            top5_acc = (top5_correct / len(labels)) * 100
            
            # è®¡ç®—AUROCï¼ˆå¤šåˆ†ç±»ä½¿ç”¨one-vs-restæ–¹å¼ï¼‰
            try:
                auroc = roc_auc_score(labels, probs, multi_class='ovr', average='macro')
            except ValueError:
                # å¦‚æœæŸäº›ç±»åˆ«æ²¡æœ‰æ ·æœ¬ï¼Œè®¾ä¸ºNone
                auroc = None
        else:
            top3_acc = None
            top5_acc = None
            auroc = None
        
        return {
            'accuracy': float(acc),
            'precision_macro': float(precision_macro * 100),
            'recall_macro': float(recall_macro * 100),
            'f1_macro': float(f1_macro * 100),
            'precision_weighted': float(precision_weighted * 100),
            'recall_weighted': float(recall_weighted * 100),
            'f1_weighted': float(f1_weighted * 100),
            'cohen_kappa': float(kappa),
            'auroc': float(auroc) if auroc is not None else None,
            'top3_accuracy': float(top3_acc) if top3_acc else None,
            'top5_accuracy': float(top5_acc) if top5_acc else None
        }
    
    # è®¡ç®—å„åˆ†æ”¯æŒ‡æ ‡
    metrics_fusion = compute_metrics(all_labels, all_preds_fusion, all_probs_fusion)
    metrics_emg = compute_metrics(all_labels, all_preds_emg, all_probs_emg)
    metrics_imu = compute_metrics(all_labels, all_preds_imu, all_probs_imu)
    
    # ==================== è®¡ç®—åˆ†ExerciseæŒ‡æ ‡ ====================
    print("è®¡ç®—åˆ†ExerciseæŒ‡æ ‡...")
    
    exercise_ids = np.unique(all_exercises)
    exercise_ids = exercise_ids[exercise_ids > 0]  # æ’é™¤é»˜è®¤å€¼0
    
    per_exercise_metrics = {}
    if len(exercise_ids) > 0:
        print(f"  å‘ç° {len(exercise_ids)} ä¸ªExercise: {exercise_ids}")
        for ex_id in exercise_ids:
            ex_mask = (all_exercises == ex_id)
            ex_labels = all_labels[ex_mask]
            ex_preds_fusion = all_preds_fusion[ex_mask]
            ex_preds_emg = all_preds_emg[ex_mask]
            ex_preds_imu = all_preds_imu[ex_mask]
            
            # è®¡ç®—F1-score
            _, _, f1_fusion, _ = precision_recall_fscore_support(
                ex_labels, ex_preds_fusion, average='macro', zero_division=0
            )
            _, _, f1_emg, _ = precision_recall_fscore_support(
                ex_labels, ex_preds_emg, average='macro', zero_division=0
            )
            _, _, f1_imu, _ = precision_recall_fscore_support(
                ex_labels, ex_preds_imu, average='macro', zero_division=0
            )
            
            per_exercise_metrics[f'E{ex_id}'] = {
                'fusion_acc': float((ex_preds_fusion == ex_labels).mean() * 100),
                'emg_acc': float((ex_preds_emg == ex_labels).mean() * 100),
                'imu_acc': float((ex_preds_imu == ex_labels).mean() * 100),
                'fusion_f1': float(f1_fusion * 100),
                'emg_f1': float(f1_emg * 100),
                'imu_f1': float(f1_imu * 100),
                'n_samples': int(len(ex_labels))
            }
            print(f"  E{ex_id}: Fusion_Acc={per_exercise_metrics[f'E{ex_id}']['fusion_acc']:.2f}%, "
                  f"Fusion_F1={per_exercise_metrics[f'E{ex_id}']['fusion_f1']:.2f}%, "
                  f"EMG_Acc={per_exercise_metrics[f'E{ex_id}']['emg_acc']:.2f}%, "
                  f"IMU_Acc={per_exercise_metrics[f'E{ex_id}']['imu_acc']:.2f}% "
                  f"({per_exercise_metrics[f'E{ex_id}']['n_samples']} samples)")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ°Exerciseä¿¡æ¯ï¼Œè·³è¿‡åˆ†Exerciseç»Ÿè®¡")
    
    # ==================== è®¡ç®—æ¨¡å‹æ•ˆç‡æŒ‡æ ‡ ====================
    print("è®¡ç®—æ¨¡å‹æ•ˆç‡æŒ‡æ ‡...")
    
    # 1. å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    emg_params = sum(p.numel() for p in model.emg_encoder.parameters()) + \
                 sum(p.numel() for p in model.emg_classifier.parameters())
    imu_params = sum(p.numel() for p in model.imu_encoder.parameters()) + \
                 sum(p.numel() for p in model.imu_classifier.parameters())
    
    # èåˆå‚æ•°ï¼ˆå…¼å®¹æ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
    if hasattr(model, 'fusion_classifier_adaptive'):
        # è‡ªé€‚åº”èåˆæ¨¡å‹
        fusion_params = (sum(p.numel() for p in model.emg_shared_encoder.parameters()) +
                        sum(p.numel() for p in model.emg_private_encoder.parameters()) +
                        sum(p.numel() for p in model.imu_shared_encoder.parameters()) +
                        sum(p.numel() for p in model.imu_private_encoder.parameters()) +
                        sum(p.numel() for p in model.adaptive_fusion.parameters()) +
                        sum(p.numel() for p in model.fusion_classifier_adaptive.parameters()))
    elif hasattr(model, 'emg_shared_encoder'):
        # è§£çº ç¼ æ¨¡å‹ï¼ˆä¸å«è‡ªé€‚åº”èåˆï¼‰
        fusion_params = (sum(p.numel() for p in model.emg_shared_encoder.parameters()) +
                        sum(p.numel() for p in model.emg_private_encoder.parameters()) +
                        sum(p.numel() for p in model.imu_shared_encoder.parameters()) +
                        sum(p.numel() for p in model.imu_private_encoder.parameters()) +
                        sum(p.numel() for p in model.fusion_classifier.parameters()))
    else:
        # åŸºç¡€æ¨¡å‹
        fusion_params = (sum(p.numel() for p in model.fusion_encoder.parameters()) +
                        sum(p.numel() for p in model.fusion_classifier.parameters()))
    
    # 2. æ¨ç†æ—¶é—´
    print("æµ‹é‡æ¨ç†æ—¶é—´ï¼ˆé¢„çƒ­+100æ¬¡æµ‹é‡ï¼‰...")
    
    # è·å–å®é™…çš„é€šé“æ•°ï¼ˆåŠ¨æ€ï¼‰
    from config import get_dataset_config
    dataset_cfg = get_dataset_config(dataset)
    emg_channels = dataset_cfg['emg_channels']
    imu_channels = dataset_cfg['imu_channels']
    
    # å‡†å¤‡ä¸€ä¸ªbatchçš„æ•°æ®ï¼ˆä½¿ç”¨è¾ƒå°çš„batché¿å…OOMï¼‰
    batch_size_test = 16  # å‡å°batch sizeé¿å…æ˜¾å­˜ä¸è¶³ï¼ˆä»64â†’16ï¼‰
    sample_created = False
    try:
        sample_emg = torch.randn(batch_size_test, 400, emg_channels, 1).to(device)
        sample_imu = torch.randn(batch_size_test, 400, imu_channels, 1).to(device)
        sample_created = True
    except RuntimeError:
        # å¦‚æœ16è¿˜ä¸å¤Ÿï¼Œé™åˆ°8
        print("âš ï¸  æ˜¾å­˜ä¸è¶³ï¼Œé™ä½batch sizeåˆ°8...")
        batch_size_test = 8
        try:
            sample_emg = torch.randn(batch_size_test, 400, emg_channels, 1).to(device)
            sample_imu = torch.randn(batch_size_test, 400, imu_channels, 1).to(device)
            sample_created = True
        except RuntimeError:
            print("âš ï¸  æ˜¾å­˜ä¸¥é‡ä¸è¶³ï¼Œè·³è¿‡æ¨ç†æ—¶é—´æµ‹é‡")
            sample_created = False
    
    # é¢„çƒ­å’Œæµ‹é‡ï¼ˆä»…åœ¨æˆåŠŸåˆ›å»ºæ ·æœ¬æ—¶è¿›è¡Œï¼‰
    if sample_created:
        try:
            with torch.no_grad():
                for _ in range(5):
                    _ = model(sample_emg, sample_imu)
            
            # æµ‹é‡
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            times = []
            with torch.no_grad():
                for _ in range(100):
                    start = time.time()
                    _ = model(sample_emg, sample_imu)
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    times.append((time.time() - start) * 1000)  # è½¬ä¸ºæ¯«ç§’
        except RuntimeError as e:
            print(f"âš ï¸  æ¨ç†æ—¶é—´æµ‹é‡å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ˜¾å­˜ä¸è¶³ï¼‰: {str(e)}")
            print("âš ï¸  ä½¿ç”¨é»˜è®¤å€¼...")
            times = [10.0]  # é»˜è®¤å€¼
            # å°è¯•æ¸…ç†å·²åˆ›å»ºçš„å˜é‡
            try:
                if 'sample_emg' in locals():
                    del sample_emg
                if 'sample_imu' in locals():
                    del sample_imu
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass  # å¿½ç•¥æ¸…ç†å¤±è´¥
    else:
        # å¦‚æœæ²¡èƒ½åˆ›å»ºæ ·æœ¬ï¼Œä½¿ç”¨é»˜è®¤å€¼
        times = [10.0]
        batch_size_test = 1  # é¿å…é™¤é›¶é”™è¯¯
    
    inference_time_mean = np.mean(times)
    inference_time_std = np.std(times)
    single_sample_time = inference_time_mean / batch_size_test  # å•æ ·æœ¬æ—¶é—´
    fps = 1000 / single_sample_time  # FPS
    
    # ç«‹å³æ¸…ç†æ¨ç†æµ‹é‡çš„æ•°æ®ï¼ˆå¦‚æœè¿˜å­˜åœ¨ï¼‰
    try:
        if 'sample_emg' in locals():
            del sample_emg
        if 'sample_imu' in locals():
            del sample_imu
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except:
        pass  # å¿½ç•¥æ¸…ç†å¤±è´¥
    
    # 3. FLOPsï¼ˆä½¿ç”¨thopåº“ï¼Œå¦‚æœå¯ç”¨ï¼‰
    flops = None
    try:
        from thop import profile
        sample_emg_single = torch.randn(1, 400, emg_channels, 1).to(device)
        sample_imu_single = torch.randn(1, 400, imu_channels, 1).to(device)
        flops, _ = profile(model, inputs=(sample_emg_single, sample_imu_single), verbose=False)
        flops = flops / 1e9  # è½¬ä¸ºGFLOPs
        # æ¸…ç†
        del sample_emg_single, sample_imu_single
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception as e:
        print(f"âš ï¸  æ— æ³•è®¡ç®—FLOPs: {str(e)}")
    
    # ==================== æ•´ç†æ‰€æœ‰æŒ‡æ ‡ ====================
    metrics = {
        'dataset': dataset,
        'subject': subject,
        'experiment_id': experiment_id,
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        
        # æ•´ä½“æ€§èƒ½
        'overall': metrics_fusion,
        
        # å„åˆ†æ”¯æ€§èƒ½
        'emg_branch': metrics_emg,
        'imu_branch': metrics_imu,
        
        # åˆ†Exerciseæ€§èƒ½
        'per_exercise': per_exercise_metrics if per_exercise_metrics else None,
        
        # æ¨¡å‹æ•ˆç‡
        'model_efficiency': {
            'total_params': int(total_params),
            'trainable_params': int(trainable_params),
            'total_params_M': float(total_params / 1e6),
            'emg_params': int(emg_params),
            'imu_params': int(imu_params),
            'fusion_params': int(fusion_params),
            'flops_G': float(flops) if flops else None,
            'inference_time_ms': float(inference_time_mean),
            'inference_time_std_ms': float(inference_time_std),
            'single_sample_time_ms': float(single_sample_time),
            'fps': float(fps),
            'batch_size': batch_size_test,
            'device': str(device)
        }
    }
    
    # ==================== æ‰“å°ç»“æœ ====================
    print("\n" + "=" * 70)
    print("ğŸ“Š æœ€ä½³æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 70)
    
    print(f"\nğŸ¯ æ•´ä½“æ€§èƒ½:")
    print(f"  Accuracy:        {metrics['overall']['accuracy']:.2f}%")
    print(f"  Precision (Macro): {metrics['overall']['precision_macro']:.2f}%")
    print(f"  Recall (Macro):    {metrics['overall']['recall_macro']:.2f}%")
    print(f"  F1-Score (Macro):  {metrics['overall']['f1_macro']:.2f}%")
    print(f"  Cohen's Kappa:   {metrics['overall']['cohen_kappa']:.4f}")
    if metrics['overall']['auroc'] is not None:
        print(f"  AUROC (Macro):   {metrics['overall']['auroc']:.4f}")
    print(f"  Top-3 Accuracy:  {metrics['overall']['top3_accuracy']:.2f}%")
    print(f"  Top-5 Accuracy:  {metrics['overall']['top5_accuracy']:.2f}%")
    
    print(f"\nğŸ”¬ å„åˆ†æ”¯æ€§èƒ½:")
    print(f"  EMGåˆ†æ”¯: Acc={metrics['emg_branch']['accuracy']:.2f}%, "
          f"F1={metrics['emg_branch']['f1_macro']:.2f}%, "
          f"Kappa={metrics['emg_branch']['cohen_kappa']:.4f}")
    print(f"  IMUåˆ†æ”¯: Acc={metrics['imu_branch']['accuracy']:.2f}%, "
          f"F1={metrics['imu_branch']['f1_macro']:.2f}%, "
          f"Kappa={metrics['imu_branch']['cohen_kappa']:.4f}")
    
    # æ‰“å°åˆ†Exerciseæ€§èƒ½
    if metrics['per_exercise']:
        print(f"\nğŸ“‹ åˆ†Exerciseæ€§èƒ½:")
        for ex_name, ex_metrics in sorted(metrics['per_exercise'].items()):
            print(f"  {ex_name}: Fusion_Acc={ex_metrics['fusion_acc']:.2f}% (F1={ex_metrics['fusion_f1']:.2f}%), "
                  f"EMG_Acc={ex_metrics['emg_acc']:.2f}% (F1={ex_metrics['emg_f1']:.2f}%), "
                  f"IMU_Acc={ex_metrics['imu_acc']:.2f}% (F1={ex_metrics['imu_f1']:.2f}%) "
                  f"({ex_metrics['n_samples']} samples)")
    
    print(f"\nâš™ï¸  æ¨¡å‹æ•ˆç‡:")
    print(f"  æ€»å‚æ•°é‡:        {metrics['model_efficiency']['total_params_M']:.2f}M")
    print(f"  å¯è®­ç»ƒå‚æ•°:      {metrics['model_efficiency']['trainable_params']:,}")
    if flops:
        print(f"  FLOPs:           {metrics['model_efficiency']['flops_G']:.2f}G")
    print(f"  æ¨ç†æ—¶é—´ (batch={metrics['model_efficiency']['batch_size']}): "
          f"{metrics['model_efficiency']['inference_time_ms']:.2f} Â± "
          f"{metrics['model_efficiency']['inference_time_std_ms']:.2f} ms")
    print(f"  å•æ ·æœ¬æ—¶é—´:      {metrics['model_efficiency']['single_sample_time_ms']:.3f} ms")
    print(f"  FPS:             {metrics['model_efficiency']['fps']:.0f}")
    
    # ==================== ä¿å­˜ç»“æœ ====================
    # åˆ›å»ºç›®å½•ç»“æ„: results/{dataset}/subject{id}/{experiment_id}/
    results_dir = Path('./results') / dataset / f'subject{subject}' / experiment_id
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜JSON
    json_path = results_dir / 'metrics.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=4, ensure_ascii=False)
    print(f"\nğŸ’¾ æŒ‡æ ‡å·²ä¿å­˜è‡³: {json_path}")
    
    # ==================== ä¿å­˜é¢„æµ‹ç»“æœï¼ˆç”¨äºèšåˆï¼‰====================
    if save_predictions:
        import pickle
        predictions_data = {
            'dataset': dataset,
            'subject': subject,
            'experiment_id': experiment_id,
            'all_labels': all_labels,
            'all_preds_fusion': all_preds_fusion,
            'all_preds_emg': all_preds_emg,
            'all_preds_imu': all_preds_imu,
            'all_probs_fusion': all_probs_fusion,
            'all_probs_emg': all_probs_emg,
            'all_probs_imu': all_probs_imu,
            'all_exercises': all_exercises
        }
        predictions_path = results_dir / 'predictions.pkl'
        with open(predictions_path, 'wb') as f:
            pickle.dump(predictions_data, f)
        print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {predictions_path} (ç”¨äºèšåˆåˆ†æ)")
    
    # ==================== ç»˜åˆ¶æ··æ·†çŸ©é˜µ ====================
    print("\nç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    
    def plot_confusion_matrix(labels, preds, title, filename, num_classes):
        cm = confusion_matrix(labels, preds)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, cmap='Blues', fmt='d', cbar=True,
                   xticklabels=range(num_classes), yticklabels=range(num_classes))
        plt.title(f'{title}\n{dataset} - Subject {subject}', fontsize=14, pad=20)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  å·²ä¿å­˜: {filename}")
    
    # è·å–å®é™…çš„ç±»åˆ«æ•°ï¼ˆä»æ•°æ®é›†é…ç½®ï¼‰
    from config import get_dataset_config
    num_classes = get_dataset_config(dataset)['num_class']
    
    plot_confusion_matrix(all_labels, all_preds_fusion, 
                         'Confusion Matrix - Fusion Model',
                         results_dir / 'confusion_matrix_fusion.png',
                         num_classes)
    
    plot_confusion_matrix(all_labels, all_preds_emg,
                         'Confusion Matrix - EMG Branch',
                         results_dir / 'confusion_matrix_emg.png',
                         num_classes)
    
    plot_confusion_matrix(all_labels, all_preds_imu,
                         'Confusion Matrix - IMU Branch',
                         results_dir / 'confusion_matrix_imu.png',
                         num_classes)
    
    # ==================== ç”ŸæˆLaTeXè¡¨æ ¼ ====================
    latex_path = results_dir / 'metrics_table.tex'
    with open(latex_path, 'w', encoding='utf-8') as f:
        f.write("% æ¨¡å‹æ€§èƒ½ä¸æ•ˆç‡å¯¹æ¯”è¡¨æ ¼\n")
        f.write("\\begin{table}[htbp]\n")
        f.write("\\centering\n")
        f.write("\\caption{Model Performance and Efficiency Metrics}\n")
        f.write("\\begin{tabular}{lcccccc}\n")
        f.write("\\hline\n")
        f.write("Branch & Acc (\\%) & F1 (\\%) & Kappa & Params (M) & Time (ms) & FPS \\\\\n")
        f.write("\\hline\n")
        f.write(f"EMG    & {metrics['emg_branch']['accuracy']:.2f} & "
                f"{metrics['emg_branch']['f1_macro']:.2f} & "
                f"{metrics['emg_branch']['cohen_kappa']:.4f} & "
                f"{emg_params/1e6:.2f} & - & - \\\\\n")
        f.write(f"IMU    & {metrics['imu_branch']['accuracy']:.2f} & "
                f"{metrics['imu_branch']['f1_macro']:.2f} & "
                f"{metrics['imu_branch']['cohen_kappa']:.4f} & "
                f"{imu_params/1e6:.2f} & - & - \\\\\n")
        f.write(f"Fusion & {metrics['overall']['accuracy']:.2f} & "
                f"{metrics['overall']['f1_macro']:.2f} & "
                f"{metrics['overall']['cohen_kappa']:.4f} & "
                f"{fusion_params/1e6:.2f} & - & - \\\\\n")
        f.write(f"Overall & {metrics['overall']['accuracy']:.2f} & "
                f"{metrics['overall']['f1_macro']:.2f} & "
                f"{metrics['overall']['cohen_kappa']:.4f} & "
                f"{metrics['model_efficiency']['total_params_M']:.2f} & "
                f"{metrics['model_efficiency']['inference_time_ms']:.2f} & "
                f"{metrics['model_efficiency']['fps']:.0f} \\\\\n")
        f.write("\\hline\n")
        f.write("\\end{tabular}\n")
        f.write("\\end{table}\n")
    print(f"ğŸ“„ LaTeXè¡¨æ ¼å·²ä¿å­˜è‡³: {latex_path}")
    
    print("\n" + "=" * 70)
    
    return metrics


def evaluate_aggregated_all_subjects(subjects, model_type, results_base_dir='./results'):
    """
    èšåˆæ‰€æœ‰å—è¯•è€…çš„è¯„ä¼°ç»“æœï¼Œç”Ÿæˆæ•´ä½“æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
    
    Args:
        subjects: å—è¯•è€…ç¼–å·åˆ—è¡¨ï¼Œä¾‹å¦‚ [1, 2, 3, ..., 40] æˆ– [10, 23, 36]
        model_type: æ¨¡å‹ç±»å‹ ("base_model", "disentangle_model", "adaptive_fusion_model")
        results_base_dir: ç»“æœåŸºç¡€ç›®å½•
    
    Returns:
        aggregated_metrics: èšåˆçš„è¯„ä¼°æŒ‡æ ‡
    """
    import pickle
    from pathlib import Path
    
    print("=" * 70)
    print("ğŸ“Š å¼€å§‹èšåˆæ‰€æœ‰å—è¯•è€…çš„è¯„ä¼°ç»“æœ...")
    print("=" * 70)
    
    # æ”¶é›†æ‰€æœ‰å—è¯•è€…çš„é¢„æµ‹ç»“æœ
    all_labels = []
    all_preds_fusion = []
    all_preds_emg = []
    all_preds_imu = []
    all_probs_fusion = []
    all_probs_emg = []
    all_probs_imu = []
    all_exercises = []
    subject_ids = []
    
    loaded_subjects = []
    missing_subjects = []
    
    for subject in subjects:
        predictions_path = Path(results_base_dir) / f'subject{subject}' / model_type / 'predictions.pkl'
        
        if not predictions_path.exists():
            missing_subjects.append(subject)
            print(f"âš ï¸  å—è¯•è€… S{subject} çš„é¢„æµ‹ç»“æœä¸å­˜åœ¨: {predictions_path}")
            continue
        
        try:
            with open(predictions_path, 'rb') as f:
                data = pickle.load(f)
            
            # æ”¶é›†æ•°æ®
            all_labels.extend(data['all_labels'])
            all_preds_fusion.extend(data['all_preds_fusion'])
            all_preds_emg.extend(data['all_preds_emg'])
            all_preds_imu.extend(data['all_preds_imu'])
            all_probs_fusion.append(data['all_probs_fusion'])
            all_probs_emg.append(data['all_probs_emg'])
            all_probs_imu.append(data['all_probs_imu'])
            all_exercises.extend(data['all_exercises'])
            
            # è®°å½•æ ·æœ¬æ¥æº
            n_samples = len(data['all_labels'])
            subject_ids.extend([subject] * n_samples)
            
            loaded_subjects.append(subject)
            print(f"âœ“ åŠ è½½å—è¯•è€… S{subject}: {n_samples} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            missing_subjects.append(subject)
            print(f"âŒ åŠ è½½å—è¯•è€… S{subject} å¤±è´¥: {str(e)}")
    
    if len(loaded_subjects) == 0:
        print("\nâŒ é”™è¯¯ï¼šæ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å—è¯•è€…çš„é¢„æµ‹ç»“æœï¼")
        print("è¯·ç¡®ä¿å·²ç»è®­ç»ƒå¹¶è¯„ä¼°äº†æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ --save-predictions å‚æ•°ä¿å­˜äº†é¢„æµ‹ç»“æœã€‚")
        return None
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_labels = np.array(all_labels)
    all_preds_fusion = np.array(all_preds_fusion)
    all_preds_emg = np.array(all_preds_emg)
    all_preds_imu = np.array(all_preds_imu)
    all_probs_fusion = np.concatenate(all_probs_fusion, axis=0)
    all_probs_emg = np.concatenate(all_probs_emg, axis=0)
    all_probs_imu = np.concatenate(all_probs_imu, axis=0)
    all_exercises = np.array(all_exercises)
    subject_ids = np.array(subject_ids)
    
    print("\n" + "=" * 70)
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(loaded_subjects)} ä¸ªå—è¯•è€…çš„é¢„æµ‹ç»“æœ")
    print(f"  æ€»æ ·æœ¬æ•°: {len(all_labels):,}")
    print(f"  åŠ è½½çš„å—è¯•è€…: {sorted(loaded_subjects)}")
    if missing_subjects:
        print(f"  ç¼ºå¤±çš„å—è¯•è€…: {sorted(missing_subjects)}")
    print("=" * 70)
    
    # ==================== è®¡ç®—èšåˆæŒ‡æ ‡ ====================
    print("\nè®¡ç®—èšåˆæ€§èƒ½æŒ‡æ ‡...")
    
    def compute_metrics(labels, preds, probs=None):
        """è®¡ç®—å•ä¸ªåˆ†æ”¯çš„æŒ‡æ ‡"""
        acc = (preds == labels).mean() * 100
        
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            labels, preds, average='macro', zero_division=0
        )
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            labels, preds, average='weighted', zero_division=0
        )
        
        kappa = cohen_kappa_score(labels, preds)
        
        if probs is not None:
            top3_correct = sum([1 if label in np.argsort(prob)[-3:] else 0 
                               for label, prob in zip(labels, probs)])
            top5_correct = sum([1 if label in np.argsort(prob)[-5:] else 0 
                               for label, prob in zip(labels, probs)])
            top3_acc = (top3_correct / len(labels)) * 100
            top5_acc = (top5_correct / len(labels)) * 100
            
            try:
                auroc = roc_auc_score(labels, probs, multi_class='ovr', average='macro')
            except ValueError:
                auroc = None
        else:
            top3_acc = None
            top5_acc = None
            auroc = None
        
        return {
            'accuracy': float(acc),
            'precision_macro': float(precision_macro * 100),
            'recall_macro': float(recall_macro * 100),
            'f1_macro': float(f1_macro * 100),
            'precision_weighted': float(precision_weighted * 100),
            'recall_weighted': float(recall_weighted * 100),
            'f1_weighted': float(f1_weighted * 100),
            'cohen_kappa': float(kappa),
            'auroc': float(auroc) if auroc is not None else None,
            'top3_accuracy': float(top3_acc) if top3_acc else None,
            'top5_accuracy': float(top5_acc) if top5_acc else None
        }
    
    metrics_fusion = compute_metrics(all_labels, all_preds_fusion, all_probs_fusion)
    metrics_emg = compute_metrics(all_labels, all_preds_emg, all_probs_emg)
    metrics_imu = compute_metrics(all_labels, all_preds_imu, all_probs_imu)
    
    # ==================== è®¡ç®—åˆ†å—è¯•è€…ç»Ÿè®¡ ====================
    print("è®¡ç®—åˆ†å—è¯•è€…ç»Ÿè®¡...")
    
    per_subject_metrics = {}
    for subject in loaded_subjects:
        mask = (subject_ids == subject)
        subj_labels = all_labels[mask]
        subj_preds_fusion = all_preds_fusion[mask]
        subj_preds_emg = all_preds_emg[mask]
        subj_preds_imu = all_preds_imu[mask]
        
        per_subject_metrics[f'S{subject}'] = {
            'fusion_acc': float((subj_preds_fusion == subj_labels).mean() * 100),
            'emg_acc': float((subj_preds_emg == subj_labels).mean() * 100),
            'imu_acc': float((subj_preds_imu == subj_labels).mean() * 100),
            'n_samples': int(len(subj_labels))
        }
    
    # è®¡ç®—å—è¯•è€…é—´çš„ç»Ÿè®¡
    fusion_accs = [m['fusion_acc'] for m in per_subject_metrics.values()]
    emg_accs = [m['emg_acc'] for m in per_subject_metrics.values()]
    imu_accs = [m['imu_acc'] for m in per_subject_metrics.values()]
    
    subject_statistics = {
        'fusion_acc_mean': float(np.mean(fusion_accs)),
        'fusion_acc_std': float(np.std(fusion_accs)),
        'fusion_acc_min': float(np.min(fusion_accs)),
        'fusion_acc_max': float(np.max(fusion_accs)),
        'emg_acc_mean': float(np.mean(emg_accs)),
        'emg_acc_std': float(np.std(emg_accs)),
        'imu_acc_mean': float(np.mean(imu_accs)),
        'imu_acc_std': float(np.std(imu_accs))
    }
    
    # ==================== æ•´ç†æ‰€æœ‰æŒ‡æ ‡ ====================
    aggregated_metrics = {
        'model_type': model_type,
        'n_subjects': len(loaded_subjects),
        'subjects': sorted(loaded_subjects),
        'missing_subjects': sorted(missing_subjects) if missing_subjects else [],
        'total_samples': int(len(all_labels)),
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        
        'overall': metrics_fusion,
        'emg_branch': metrics_emg,
        'imu_branch': metrics_imu,
        
        'per_subject': per_subject_metrics,
        'subject_statistics': subject_statistics
    }
    
    # ==================== æ‰“å°ç»“æœ ====================
    print("\n" + "=" * 70)
    print("ğŸ“Š èšåˆè¯„ä¼°ç»“æœ")
    print("=" * 70)
    
    print(f"\nğŸ¯ æ•´ä½“æ€§èƒ½ (åŸºäº {len(all_labels):,} ä¸ªæ ·æœ¬):")
    print(f"  Accuracy:          {metrics_fusion['accuracy']:.2f}%")
    print(f"  Precision (Macro): {metrics_fusion['precision_macro']:.2f}%")
    print(f"  Recall (Macro):    {metrics_fusion['recall_macro']:.2f}%")
    print(f"  F1-Score (Macro):  {metrics_fusion['f1_macro']:.2f}%")
    print(f"  Cohen's Kappa:     {metrics_fusion['cohen_kappa']:.4f}")
    if metrics_fusion['auroc'] is not None:
        print(f"  AUROC (Macro):     {metrics_fusion['auroc']:.4f}")
    print(f"  Top-3 Accuracy:    {metrics_fusion['top3_accuracy']:.2f}%")
    print(f"  Top-5 Accuracy:    {metrics_fusion['top5_accuracy']:.2f}%")
    
    print(f"\nğŸ”¬ å„åˆ†æ”¯æ€§èƒ½:")
    print(f"  EMGåˆ†æ”¯:  Acc={metrics_emg['accuracy']:.2f}%, F1={metrics_emg['f1_macro']:.2f}%, Kappa={metrics_emg['cohen_kappa']:.4f}")
    print(f"  IMUåˆ†æ”¯:  Acc={metrics_imu['accuracy']:.2f}%, F1={metrics_imu['f1_macro']:.2f}%, Kappa={metrics_imu['cohen_kappa']:.4f}")
    
    print(f"\nğŸ“ˆ å—è¯•è€…é—´ç»Ÿè®¡ (åŸºäº {len(loaded_subjects)} ä¸ªå—è¯•è€…):")
    print(f"  Fusionå‡†ç¡®ç‡: {subject_statistics['fusion_acc_mean']:.2f}% Â± {subject_statistics['fusion_acc_std']:.2f}%")
    print(f"    èŒƒå›´: [{subject_statistics['fusion_acc_min']:.2f}%, {subject_statistics['fusion_acc_max']:.2f}%]")
    print(f"  EMGå‡†ç¡®ç‡:    {subject_statistics['emg_acc_mean']:.2f}% Â± {subject_statistics['emg_acc_std']:.2f}%")
    print(f"  IMUå‡†ç¡®ç‡:    {subject_statistics['imu_acc_mean']:.2f}% Â± {subject_statistics['imu_acc_std']:.2f}%")
    
    # ==================== ä¿å­˜ç»“æœ ====================
    aggregated_dir = Path(results_base_dir) / 'aggregated' / model_type
    aggregated_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜JSON
    json_path = aggregated_dir / 'aggregated_metrics.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(aggregated_metrics, f, indent=4, ensure_ascii=False)
    print(f"\nğŸ’¾ èšåˆæŒ‡æ ‡å·²ä¿å­˜è‡³: {json_path}")
    
    # ==================== ç»˜åˆ¶èšåˆæ··æ·†çŸ©é˜µ ====================
    print("\nç»˜åˆ¶èšåˆæ··æ·†çŸ©é˜µ...")
    
    def plot_aggregated_confusion_matrix(labels, preds, title, filename):
        cm = confusion_matrix(labels, preds)
        
        plt.figure(figsize=(14, 12))
        sns.heatmap(cm, cmap='Blues', fmt='d', cbar=True,
                   xticklabels=range(50), yticklabels=range(50))
        
        # æ·»åŠ æ ·æœ¬æ•°ä¿¡æ¯
        total_samples = len(labels)
        n_subjects = len(loaded_subjects)
        plt.title(f'{title}\n{n_subjects} Subjects, {total_samples:,} Samples', 
                 fontsize=14, pad=20)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  å·²ä¿å­˜: {filename}")
    
    plot_aggregated_confusion_matrix(
        all_labels, all_preds_fusion,
        'Aggregated Confusion Matrix - Fusion Model',
        aggregated_dir / 'confusion_matrix_fusion.png'
    )
    
    plot_aggregated_confusion_matrix(
        all_labels, all_preds_emg,
        'Aggregated Confusion Matrix - EMG Branch',
        aggregated_dir / 'confusion_matrix_emg.png'
    )
    
    plot_aggregated_confusion_matrix(
        all_labels, all_preds_imu,
        'Aggregated Confusion Matrix - IMU Branch',
        aggregated_dir / 'confusion_matrix_imu.png'
    )
    
    # ==================== ç»˜åˆ¶å½’ä¸€åŒ–æ··æ·†çŸ©é˜µï¼ˆå¯é€‰ï¼‰====================
    print("\nç»˜åˆ¶å½’ä¸€åŒ–æ··æ·†çŸ©é˜µï¼ˆæŒ‰è¡Œå½’ä¸€åŒ–ï¼‰...")
    
    def plot_normalized_confusion_matrix(labels, preds, title, filename):
        cm = confusion_matrix(labels, preds)
        # æŒ‰è¡Œå½’ä¸€åŒ–ï¼ˆæ¯ä¸€è¡Œä»£è¡¨çœŸå®ç±»åˆ«çš„é¢„æµ‹åˆ†å¸ƒï¼‰
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        plt.figure(figsize=(14, 12))
        sns.heatmap(cm_normalized, cmap='Blues', fmt='.2f', cbar=True,
                   xticklabels=range(50), yticklabels=range(50),
                   vmin=0, vmax=1)
        
        total_samples = len(labels)
        n_subjects = len(loaded_subjects)
        plt.title(f'{title} (Row-Normalized)\n{n_subjects} Subjects, {total_samples:,} Samples', 
                 fontsize=14, pad=20)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  å·²ä¿å­˜: {filename}")
    
    plot_normalized_confusion_matrix(
        all_labels, all_preds_fusion,
        'Aggregated Confusion Matrix - Fusion Model',
        aggregated_dir / 'confusion_matrix_fusion_normalized.png'
    )
    
    # ==================== ç”ŸæˆLaTeXè¡¨æ ¼ ====================
    latex_path = aggregated_dir / 'aggregated_metrics_table.tex'
    with open(latex_path, 'w', encoding='utf-8') as f:
        f.write("% èšåˆæ¨¡å‹æ€§èƒ½è¡¨æ ¼\n")
        f.write("\\begin{table}[htbp]\n")
        f.write("\\centering\n")
        f.write(f"\\caption{{Aggregated Performance across {len(loaded_subjects)} Subjects}}\n")
        f.write("\\begin{tabular}{lcccc}\n")
        f.write("\\hline\n")
        f.write("Branch & Acc (\\%) & F1 (\\%) & Kappa & Samples \\\\\n")
        f.write("\\hline\n")
        f.write(f"EMG    & {metrics_emg['accuracy']:.2f} & "
                f"{metrics_emg['f1_macro']:.2f} & "
                f"{metrics_emg['cohen_kappa']:.4f} & {len(all_labels):,} \\\\\n")
        f.write(f"IMU    & {metrics_imu['accuracy']:.2f} & "
                f"{metrics_imu['f1_macro']:.2f} & "
                f"{metrics_imu['cohen_kappa']:.4f} & {len(all_labels):,} \\\\\n")
        f.write(f"Fusion & {metrics_fusion['accuracy']:.2f} & "
                f"{metrics_fusion['f1_macro']:.2f} & "
                f"{metrics_fusion['cohen_kappa']:.4f} & {len(all_labels):,} \\\\\n")
        f.write("\\hline\n")
        f.write("\\multicolumn{5}{l}{\\textit{Subject Statistics (Fusion):}} \\\\\n")
        f.write(f"Mean $\\pm$ Std & {subject_statistics['fusion_acc_mean']:.2f} $\\pm$ {subject_statistics['fusion_acc_std']:.2f} & - & - & {len(loaded_subjects)} subjects \\\\\n")
        f.write("\\hline\n")
        f.write("\\end{tabular}\n")
        f.write("\\end{table}\n")
    print(f"ğŸ“„ LaTeXè¡¨æ ¼å·²ä¿å­˜è‡³: {latex_path}")
    
    print("\n" + "=" * 70)
    print(f"âœ… èšåˆè¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {aggregated_dir}")
    print("=" * 70)
    
    return aggregated_metrics

